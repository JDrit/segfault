%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Professional Newsletter Template
% LaTeX Template
% Version 1.0 (09/03/14)
%
% Created by:
% Bob Kerstetter (https://www.tug.org/texshowcase/) and extensively modified by:
% Vel (vel@latextemplates.com)
% 
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[9pt]{extarticle} % The default font size is 10pt; 11pt and 12pt are alternatives

\input{structure.tex} % Include the document which specifies all packages and structural customizations for this template

\begin{document}

%--------------------------------------------------------------------------------
% HEADER DETAILS
%--------------------------------------------------------------------------------

\pagestyle{fancy}
\fancyhf{}
\chead{segfault@csh.rit.edu}
\rhead{\today}
\lhead{Volume XLVIII Issue \#5}
\addtolength\footskip{-15px}
\cfoot{"Wow coffee is a pretty natural femdom" - John King (johnsk) }
\setlength{\headsep}{0.1in}

%----------------------------------------------------------------------------------------
%	HEADER IMAGE
%----------------------------------------------------------------------------------------

\begin{figure}[H]
\centering\vspace{0.5cm}\includegraphics[width=0.8\linewidth]{imgs/segfault.png}
\end{figure}

%--------------------------------------------------------------------------------
% HEADER QUOTE
%--------------------------------------------------------------------------------

\vspace{-15px}
\begin{quote}
\centering
\textbf{\textit{Yes I do mine your data}}
\end{quote}
\vspace{10px}

%----------------------------------------------------------------------------------------
%	SIDEBAR - FIRST PAGE
%----------------------------------------------------------------------------------------

\vspace{-0.5cm}\begin{minipage}[t]{.40\linewidth} % Mini page taking up 35% of the actual page
\begin{mdframed}[style=sidebar,frametitle={}] % Sidebar box

%-----------------------------------------------------------

\hypertarget{contents}{\textbf{{\large This week on floor\ldots}}} % \hypertarget provides a label to reference using \hyperlink{label}{link text}
\begin{itemize}
\item \hyperlink{firstnews}{Jaccard Distance Metric}
\item \hyperlink{secondnews}{NNTP Data Mining}
\end{itemize}

\centerline {\rule{.75\linewidth}{.25pt}} % Horizontal line

%-----------------------------------------------------------

\textbf{Notable Upcoming Events:}
\begin{enumerate}[leftmargin=0.2cm]
\item \textbf{History Discussion} Feb. 23, 9pm \\
	This week's discussion is going to be about either the future of drink or about showcasing CSH diversity on CSH's website.
\\
\item \textbf{Shoyler's Movie Night} Feb. 23, 10:30pm \\
	Come watch another amazing movie with your host Shoyler Martin!!!
\\
\item \textbf{Tea in the Lounge} Feb. 25, 4:45pm \\
	John King and Buster are once again hosting their weekly tea event.
\\
\item \textbf{Wings with Alum} Feb. 24, 9pm \\
	Reed is organizing "Wings over Wednesday" but with a twist!! The CSH-East (Boston) group will be calling in remote to hang out with people. Come eat amazing wings and meet up with alumni.
\\
\item \textbf{Intuit Workshop} Feb. 26, 5pm \\
	Intuit is coming to floor to do stuff...
\\
\item \textbf{Until Dawn Walkthrough} Feb. 27, 7pm \\
	Social is hosting a walkthrough of the game. Come get your scare on.
\\
\item \textbf{CSH Jeopardy} Feb. 28, 4pm \\
	Braden is hosting a Jeopardy game, come join him for a ton of fun and HR violations. 
\\
\end{enumerate}

%-----------------------------------------------------------

\textbf{Death by Dagger} \\
JD is hosting this semester's game of Death by Dagger. It will start next Sunday at midnight. The buy-in is \$1, so please give it to JD before Sunday night.

\textbf{CoreDump} \\
Braden is in charge of Coredump this year. All E-Board members make sure that you submit your articles to him shortly.

%-----------------------------------------------------------

\textbf{Joke of the Week} \\
"If you put a million monkeys at a million keyboards, one of them will eventually write a Java program.

The rest of them will all write Perl programs."
\\

\textbf{Amendment Vote}:
There were ballots distributed tonight about changing the process of voting for Spring evaluations. Remember to vote and if you give your ballot to Reed, you are literally human garbage.
\\
%-----------------------------------------------------------

\captionof*{table}{Voting Results}
\begin{tabular}{lcr}

Vote & Cost & Result \\
\midrule
Paper Towels for Imps & \$40 & PASSED \\
\bottomrule
\end{tabular}

%-----------------------------------------------------------

\end{mdframed}
\end{minipage}\hfill % End the sidebar mini page 
%
%----------------------------------------------------------------------------------------
%	MAIN BODY - FIRST PAGE
%----------------------------------------------------------------------------------------
%
\begin{minipage}[t]{.56\linewidth} % Mini page taking up 61% of the actual page
\vspace{-0.4cm}
\hypertarget{firstnews}{\heading{Jaccard Distance Metric}{6pt}}

The Jaccard index, also known as the Jaccard similarity coefficient (originally coined coefficient de communaut√© by Paul Jaccard), is a statistical method that is commonly used for comparing the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets: \\
\\
\begingroup
    \fontsize{18pt}{12pt}\selectfont
	\centerline{d(x, y) = $\frac{\vert\ x\ \cap\ y\ \vert}{\vert\ x\ \cup\ y\ \vert}$}
\endgroup
\\

An important class of problems that Jaccard addresses well is that of finding textually similar documents in a large corpus such as the Web or a collection of news articles. Many of these involve finding duplicates or near duplicated. First, let us observe that testing whether two documents are exact duplicates is easy; just compare the two documents character-by-character. However, in many applications, the documents are not identical, yet they share large portions of their text. Here are some examples: \\
\\
\textbf{Plagiarism:} \\
Finding plagiarized documents tests our ability to find textual similarity. The plagiarizer may extract only some parts of a document for his own. He may alter a few words and may alter the order in which the sentences appear. No simple process of comparing documents character-by-character will detect sophisticated plagiarism. \\
\\
\textbf{Mirror Page:} \\
It is common for important or popular web sites to be duplicated at a number of hosts, in order to share the load. The pages of these \textit{mirror} sites will be quite similar, but are rarely identical. For instance, they might each contain information associated with their particular host, and they might contain links to the other mirror sites but not to themselves. It is important to be able to detect similar pages of these kinds, because search engines produce better results if they avoid showing two pages that are nearly identical within the first page of results. \\
\\
\textbf{Articles from the Same Source:} \\
It is common for one reporter to write a news article that gets distributed say through the Associated Press, to many newspapers, which then publish the article on their sites. Each newspaper changes the article somewhat. They may cut out paragraphs, or even add material of their own. they most likely will surround the article with their own logo or adds. News aggregators try to find all versions of such an article, in order to show only one, and that task requires finding when two web pages are textually similar, although not identical.  
\\
\\
Since the formula is relatively computationally simple, it works well for datasets. There have been many extensions to this algorithm, but that goes beyond the scope discussed here.

%-----------------------------------------------------------


\end{minipage} % End the main body - first page mini page

\pagebreak

\hypertarget{secondnews}{\heading{NNTP Data Mining}{6pt}} 

Some people wanted me to post the results of my data mining project of our NNTP server, so here is the abstract.  \\
\\
Our server has over 125,000 posts but I only used the last 3 years of data, which was 8,145 posts. Data mining was done using agglomerative clustering over the body of the posts. Text-mining can be difficult to deal with since it cannot be represented in Euclidean space. To get around this, Jaccard similarity coefficient was used as the distance metric since it is able to effectively find similar data. It produces a   value between one and zero where zero means the two items are identical and one means they have nothing in common. Preprocessing was done on the data to remove stop words, normalize the data, and to preform dimensionality reduction. \\
\\
\centerline{\textbf{Results}}

\begin{multicols}{2}

\centerline{\includegraphics[width=\linewidth]{imgs/2016-02-15-clusters.png}}

As shown here, the biggest cluster was a combination of all the meeting notes posted by our glorious e-board members. This cluster was easy to find since most of our e-board members post notes following a very similar format and talk about similar material. An interesting side note is that \textbf{Marck} Billow generated a cluster of his own due to how unique his meeting notes are. He just had to be special...

\centerline{\includegraphics[width=\linewidth]{imgs/2016-02-15-users.png}}

Here are the most active users of our NNTP server. As shown here, our top 12 users accounted for 37.58\% of the total posts in the last 3 years. \\
\\


\vfill
\columnbreak

\centerline{\includegraphics[width=\linewidth]{imgs/2016-02-15-error.png}}

The error rate of a cluster was generated by the average distance between every post in the cluster. This forms a value between 1 and 0, in which 0 means all posts are identical and 1 means that there are no shared similarities. The results from this graph make sense since the birthday bot posts very similarly formatted messages so that cluster has the lowest differences between posts. Weekly food orders tend to involve the same people and all be about the same topic so that cluster also has a low error rate. 



Let me know if you have additional questions about my findings. 

\end{multicols}

\end{document} 